import json
import os
import datetime
import chromadb
import uuid
from openai import OpenAI
from dotenv import load_dotenv

# --- ×”×’×“×¨×•×ª × ×ª×™×‘×™× ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "..", "data")
MEMORY_PATH = os.path.join(DATA_DIR, "memory.json")
# ×›××Ÿ ×™×™×©××¨ ×”××•×— ×”×•×§×˜×•×¨×™ (×”×ª×™×§×™×™×” ×”×—×“×©×”)
DB_PATH = os.path.join(DATA_DIR, "brain_db") 
ENV_PATH = os.path.join(BASE_DIR, ".env")
load_dotenv(ENV_PATH)

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ×•×•×“× ×©×”×ª×™×§×™×™×” ×§×™×™××ª
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- ××ª×—×•×œ ChromaDB (×”××•×— ×”×•×§×˜×•×¨×™) ---
# ×–×” ×™×•×¦×¨ ××•×˜×•××˜×™×ª ××ª ×”×§×‘×¦×™× ×”×“×¨×•×©×™× ×‘×ª×™×§×™×™×ª data/brain_db
try:
    chroma_client = chromadb.PersistentClient(path=DB_PATH)
    # ××•×¡×£ ×”×¢×•×‘×“×•×ª (×›××• ×˜×‘×œ×” ×‘×‘×¡×™×¡ × ×ª×•× ×™×)
    facts_collection = chroma_client.get_or_create_collection("facts")
    print("ğŸ§  ChromaDB Vector Engine Initialized Successfully.")
except Exception as e:
    print(f"âš ï¸ Failed to initialize ChromaDB: {e}")
    facts_collection = None

# --- ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ×œ×§×•×‘×¥ JSON (×–×™×›×¨×•×Ÿ ×œ×˜×•×•×— ×§×¦×¨) ---
def _load_memory():
    """×˜×•×¢×Ÿ ××ª ×§×•×‘×¥ ×”×–×™×›×¨×•×Ÿ (×¨×§ ×œ×©×™×—×” ×©×•×˜×¤×ª)"""
    if not os.path.exists(MEMORY_PATH):
        return {"conversations": []}
    try:
        with open(MEMORY_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {"conversations": []}

def _save_memory_file(data):
    try:
        with open(MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"Error saving memory file: {e}")

# --- ×”×¤×•× ×§×¦×™×•×ª ×”×¨××©×™×•×ª ---

def save_memory(content, importance="medium"):
    """
    ×©××™×¨×” ×™×“× ×™×ª (REMEMBER) ×œ××¡×“ ×”×•×§×˜×•×¨×™.
    ×–×” × ×©××¨ ×œ× ×¦×— ×‘-ChromaDB.
    """
    if not facts_collection:
        return "×©×’×™××”: ××¡×“ ×”× ×ª×•× ×™× ×œ× ×–××™×Ÿ."

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    doc_id = str(uuid.uuid4())
    
    try:
        facts_collection.add(
            documents=[content],
            metadatas=[{"timestamp": timestamp, "type": "manual_fact", "importance": importance}],
            ids=[doc_id]
        )
        return f"× ×©××¨ ×‘×–×™×›×¨×•×Ÿ ×”×˜×•×•×— ×”××¨×•×š: {content}"
    except Exception as e:
        return f"×©×’×™××” ×‘×©××™×¨×”: {e}"

def retrieve_memory(query, n_results=5):
    """
    ×©×œ×™×¤×” ×—×›××” (RAG):
    1. ××—×¤×© ×¢×•×‘×“×•×ª ×¨×œ×•×•× ×˜×™×•×ª ×‘-ChromaDB (×œ×¤×™ ×“××™×•×Ÿ ×¡×× ×˜×™).
    2. ××•×©×š ××ª ×¡×•×£ ×”×©×™×—×” ××§×•×‘×¥ ×”-JSON.
    """
    facts_str = "No relevant long-term facts found."
    
    # 1. ×—×™×¤×•×© ×‘-ChromaDB
    if facts_collection:
        try:
            results = facts_collection.query(
                query_texts=[query],
                n_results=n_results
            )
            
            if results['documents'] and results['documents'][0]:
                found_facts = results['documents'][0]
                # ×× ×§×” ×›×¤×™×œ×•×™×•×ª ×•××¡×“×¨ ×™×¤×”
                facts_str = "\n".join([f"- {fact}" for fact in found_facts])
        except Exception as e:
            print(f"Vector search error: {e}")

    # 2. ×©×œ×™×¤×ª ×”×©×™×—×” ×”××—×¨×•× ×” (Context) ××§×•×‘×¥ ×”-JSON
    data = _load_memory()
    # ×œ×•×§×—×™× ×¨×§ ××ª ×”-10 ×”××—×¨×•× ×•×ª ×›×“×™ ×œ×ª×ª ×”×§×©×¨ ××™×™×“×™
    recent_convo = data.get("conversations", [])[-10:]
    convo_str = json.dumps(recent_convo, ensure_ascii=False)
    
    return f"Long-Term Memory (Facts):\n{facts_str}\n\nShort-Term Memory (Recent Chat):\n{convo_str}"

def save_episode(description, user_emotion, ai_emotion, importance="medium"):
    """×©××™×¨×ª ××¤×™×–×•×“×” - × ×›× ×¡ ×’× ×œ-ChromaDB"""
    content = f"Episode: {description} | User Emotion: {user_emotion}"
    return save_memory(content, importance)

# --- ×”×× ×•×¢ ×”×—×“×©: Consolidation ×œ×ª×•×š ChromaDB ---
def consolidate_memory():
    """
    ×”×¤×•× ×§×¦×™×” ×©×¨×¦×” ×‘×—×œ×•×:
    1. ×œ×•×§×—×ª ×©×™×—×•×ª ×™×©× ×•×ª ×-JSON.
    2. ××—×œ×¦×ª ×¢×•×‘×“×•×ª ×‘×¢×–×¨×ª GPT.
    3. ×“×•×—×¤×ª ××ª ×”×¢×•×‘×“×•×ª ×œ-ChromaDB (× ×¦×—).
    4. ××•×—×§×ª ××ª ×”×©×™×—×•×ª ×”×™×©× ×•×ª ××”-JSON.
    """
    data = _load_memory()
    conversations = data.get("conversations", [])
    
    # ×¡×£ ×”× ×™×§×•×™: 60 ×”×•×“×¢×•×ª (×›×“×™ ×œ× ×œ× ×§×•×ª ×›×œ ×¨×’×¢)
    if len(conversations) < 60:
        return
        
    print("ğŸ§  ××‘×¦×¢ ×ª×”×œ×™×š ×’×™×‘×•×© ×–×™×›×¨×•×Ÿ ×œ×˜×•×•×— ××¨×•×š (ChromaDB)...")
    
    # ××©××™×¨×™× ××ª ×”-50 ×”××—×¨×•× ×•×ª ×‘×–×™×›×¨×•×Ÿ ×¢×‘×•×“×” (Short Term)
    to_analyze = conversations[:-50] 
    to_keep = conversations[-50:]
    
    conversation_text = json.dumps(to_analyze, ensure_ascii=False)
    
    prompt = f"""
    Analyze this conversation log between AI and User (Maor).
    Extract clear, distinct FACTS about the user (preferences, hobbies, work, pets, location, plans).
    Ignore small talk.
    Return a list of facts in Hebrew. 
    If nothing new was learned, return "NO_NEW_INFO".
    Log: {conversation_text}
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        result = response.choices[0].message.content.strip()
        
        if "NO_NEW_INFO" not in result and facts_collection:
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
            new_facts = result.split("\n")
            
            # ×”×›× ×” ×œ×”×•×¡×¤×” ×‘×‘×ª ××—×ª (Batch)
            docs = []
            metas = []
            ids = []
            
            for fact in new_facts:
                clean_fact = fact.replace("- ", "").strip()
                # ×¡×™× ×•×Ÿ ×–×‘×œ (×©×•×¨×•×ª ×¨×™×§×•×ª ××• ×§×¦×¨×•×ª ××“×™)
                if clean_fact and len(clean_fact) > 5:
                    docs.append(clean_fact)
                    metas.append({"timestamp": timestamp, "type": "consolidated_fact"})
                    ids.append(str(uuid.uuid4()))
                    print(f"ğŸ’¡ × ×œ××“ ×•× ×©××¨ ×‘-ChromaDB: {clean_fact}")
            
            if docs:
                facts_collection.add(documents=docs, metadatas=metas, ids=ids)
        
        # ××—×™×§×ª ×”×”×™×¡×˜×•×¨×™×” ×”×™×©× ×” ××”-JSON
        data["conversations"] = to_keep
        _save_memory_file(data)
        print("âœ… ×”×–×™×›×¨×•×Ÿ ×¢×‘×¨ ××•×¤×˜×™××™×–×¦×™×”: ×”×•×¢×‘×¨ ×œ-Vector DB.")
        
    except Exception as e:
        print(f"Consolidation Error: {e}")

# ×œ×‘×“×™×§×” ×™×“× ×™×ª
if __name__ == "__main__":
    consolidate_memory()