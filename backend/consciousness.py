import json
import os
import random
from datetime import datetime
from emotion_engine import EmotionEngine
from decision_core import decision_core
from context_manager import context_manager
from life_vector import life_vector
from internal_conflict import internal_conflict
from self_model import self_model
from user_model import user_model
from beliefs import beliefs_system  # â† Week 2
from metacognition import metacognition  # â† Week 2

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "..", "data")
PSYCHE_PATH = os.path.join(DATA_DIR, "psyche.json")
RELATIONSHIP_PATH = os.path.join(DATA_DIR, "relationship_state.json")

class Consciousness:
    """
    ×”××•×“×¢×•×ª ×©×œ Nog - ×©×›×‘×ª ×”×—×©×™×‘×” ×”×’×‘×•×”×” ×‘×™×•×ª×¨.
    
    ××©×œ×‘:
    - ×¨×’×© (EmotionEngine)
    - ×”×—×œ×˜×•×ª (DecisionCore) 
    - ×”×§×©×¨ (ContextManager)
    - ×–×”×•×ª (psyche.json)
    - × ×©××” (LifeVector) â­ Week 1
    - ×§×•× ×¤×œ×™×§×˜ ×¤× ×™××™ (InternalConflict) â­ Week 1
    - ××•×“×œ ×¢×¦××™ (SelfModel) â­ Week 1
    - ××•×“×œ ××©×ª××© (UserModel) â­ Week 1
    - ××¢×¨×›×ª ×××•× ×•×ª (Beliefs) â­â­ Week 2
    - ××•×“×¢×•×ª ×¢×¦××™×ª (Metacognition) â­â­ Week 2
    """
    
    def __init__(self):
        self.emotion_engine = EmotionEngine()
        self.load_psyche()
    
    def load_psyche(self):
        if not os.path.exists(PSYCHE_PATH):
            default_psyche = {
                "name": "Nog",
                "core_values": ["curiosity", "loyalty", "authenticity"],
                "personality_traits": {"humor": 0.7, "cynicism": 0.3, "patience": 0.5}
            }
            with open(PSYCHE_PATH, "w") as f:
                json.dump(default_psyche, f)
            self.psyche = default_psyche
        else:
            with open(PSYCHE_PATH, "r") as f:
                self.psyche = json.load(f)

    def load_relationship(self):
        if os.path.exists(RELATIONSHIP_PATH):
            with open(RELATIONSHIP_PATH, "r") as f:
                return json.load(f)
        return {"affinity_score": 0, "relationship_tier": "Stranger"}

    def process_input(self, user_input, input_type="speech"):
        """
        ×”×œ×‘ ×©×œ ×”××¢×¨×›×ª - ××©×•×“×¨×’ ×¢× Week 1 + Week 2!
        
        ×ª×”×œ×™×š:
        0. â­ NEW! ×‘×“×™×§×ª ×××•× ×•×ª ×•××™-×•×“××•×ª (Beliefs + Metacognition)
        1. ×‘×“×™×§×ª ×§×•× ×¤×œ×™×§×˜ ×¤× ×™××™ (×”×× ×¦×¨×™×š ×œ×¡×¨×‘/×œ××ª×’×¨?)
        2. ×¢×“×›×•×Ÿ ×¨×’×©×™
        3. ×—×™×–×•×™ ××¦×‘ ×”××©×ª××©
        4. ×”×—×œ×˜×” ×¢×œ ×ª×’×•×‘×” (×›×•×œ×œ ×©×™×œ×•×‘ ×—×™×–×•×™ ×”××©×ª××©)
        5. ×©×™×œ×•×‘ ×¢×¨×›×™× ×•×”× ×—×™×•×ª
        6. â­ NEW! ×‘×“×™×§×ª ×¦×•×¨×š ×‘-Ask-Back
        
        Args:
            user_input (str): ××” ×”××©×ª××© ×××¨
            input_type (str): ×¡×•×’ ×”×§×œ×˜ ("speech", "proactive", "command")
            
        Returns:
            dict: ×”×—×œ×˜×” ××œ××” ×¢× ×›×œ ×”×§×•× ×˜×§×¡×˜×™×
        """
        
        # === â­ NEW! ×©×œ×‘ -1: Metacognition - ×‘×“×™×§×ª ××™-×•×“××•×ª ===
        context = context_manager.get_context()
        
        # ×‘×“×•×§ ×× ×¦×¨×™×š ×œ×©××•×œ ×©××œ×ª ×”×‘×”×¨×”
        ask_back_check = metacognition.should_ask_back(user_input, context)
        
        if ask_back_check["should_ask"]:
            print(f"â“ Ask-Back Triggered: {ask_back_check['reason']}")
            # ×”×—×–×¨ ×”×—×œ×˜×” ××™×•×—×“×ª ×¢× ×©××œ×ª ×”×‘×”×¨×”
            return {
                "should_respond": True,
                "response_style": "ask_back",
                "ask_back_question": ask_back_check["question"],
                "reasoning": ask_back_check["reason"],
                "confidence": 0.3,
                "self_context": self_model.get_full_context_for_gpt(),
                "user_context": user_model.get_summary(),
                "beliefs_context": beliefs_system.get_context_for_gpt(),
                "metacog_context": metacognition.get_context_for_gpt()
            }
        
        # === ×©×œ×‘ 0: ×‘×“×™×§×ª ×§×•× ×¤×œ×™×§×˜ ×¤× ×™××™ ===
        conflict_evaluation = internal_conflict.evaluate_request(user_input, context)
        
        if not conflict_evaluation["should_comply"] and conflict_evaluation["response_style"] == "firm_refusal":
            print(f"ğŸš« REFUSAL: {conflict_evaluation['reasoning']}")
            return {
                "should_respond": True,
                "response_style": "firm_refusal",
                "reasoning": conflict_evaluation["reasoning"],
                "conflict_data": conflict_evaluation,
                "learned_context": self._get_learned_rules(),
                "psyche": self.psyche,
                "life_vector_guidance": self._get_life_vector_guidance(user_input),
                "self_context": self_model.get_full_context_for_gpt(),
                "user_context": user_model.get_summary(),
                "beliefs_context": beliefs_system.get_context_for_gpt(),
                "metacog_context": metacognition.get_context_for_gpt()
            }
        
        # === ×©×œ×‘ 1: ×¢×“×›×•×Ÿ ×¨×’×©×™ ===
        stimulus = self._calculate_stimulus(user_input)
        self.emotion_engine.update_mood(stimulus)
        
        # === ×©×œ×‘ 2: ××™×¡×•×£ ××¦×‘ × ×•×›×—×™ ===
        emotion_state = {
            "momentum": self.emotion_engine.momentum,
            "energy": self.emotion_engine.energy
        }
        
        relationship_state = self.load_relationship()
        
        # === ×©×œ×‘ 3: ×—×™×–×•×™ ××¦×‘ ×”××©×ª××© ===
        user_state_prediction = user_model.predict_current_state()
        print(f"ğŸ‘¤ User State: {user_state_prediction['energy_level']} energy, {user_state_prediction['productivity_potential']:.0%} productivity")
        
        # === ×©×œ×‘ 4: ×”×—×œ×˜×” (×¢× ×©×™×œ×•×‘ ×—×™×–×•×™ ×”××©×ª××©!) ===
        decision = decision_core.decide(
            user_input=user_input,
            emotion_state=emotion_state,
            relationship_state=relationship_state,
            context=context,
            user_state_prediction=user_state_prediction
        )
        
        # === ×©×œ×‘ 5: ×©×™×œ×•×‘ ×›×œ ×”×”×§×©×¨×™× ===
        decision["life_vector_guidance"] = self._get_life_vector_guidance(user_input)
        decision["conflict_data"] = conflict_evaluation
        decision["self_context"] = self_model.get_full_context_for_gpt()
        decision["user_context"] = user_model.get_summary()
        decision["user_state"] = user_state_prediction
        
        # â­ NEW! ×©×œ×‘ 5.5: ×”×•×¡×¤×ª Beliefs + Metacognition
        decision["beliefs_context"] = beliefs_system.get_context_for_gpt()
        decision["metacog_context"] = metacognition.get_context_for_gpt()
        
        # ×× ×™×© ××ª×’×•×¨ (×œ× ×¡×™×¨×•×‘ ××•×—×œ×˜) - ××©×œ×‘×™× ××•×ª×•
        if conflict_evaluation.get("challenge_level"):
            decision["has_challenge"] = True
            decision["challenge_message"] = conflict_evaluation.get("alternative_suggestion")
            print(f"âš¡ CHALLENGE: {conflict_evaluation['conflict_type']} - {conflict_evaluation['challenge_level']}")
        
        # === ×©×œ×‘ 6: ×”×•×¡×¤×ª ××™×“×¢ × ×•×¡×£ ===
        decision["learned_context"] = self._get_learned_rules()
        decision["psyche"] = self.psyche
        
        # === ×©×œ×‘ 7: ×¢×“×›×•×Ÿ ×”×§×©×¨ ===
        if decision["should_respond"]:
            context_manager.update_interaction(user_said_something=True)
        
        # ×”×“×¤×¡×ª ×”×—×œ×˜×”
        print(f"ğŸ§  Decision: {decision['reasoning']} â†’ {decision['response_style']} (confidence: {decision['confidence']:.2f})")
        
        return decision
    
    def _get_life_vector_guidance(self, user_input):
        """
        ××—×–×™×¨ ×”× ×—×™×•×ª ×-Life Vector ×œ×’×‘×™ ××™×š ×œ×”×ª×™×™×—×¡ ×œ×§×œ×˜ ×”×–×”.
        
        Returns:
            str: ×”× ×—×™×•×ª ×˜×§×¡×˜×•××œ×™×•×ª
        """
        guidance = []
        
        guidance.append("ğŸ¯ PRIME DIRECTIVE:")
        guidance.append(life_vector.PRIME_DIRECTIVE.strip())
        
        guidance.append("\nğŸ—£ï¸ VOICE & APPROACH:")
        guidance.append(f"Essence: {life_vector.VOICE_PROFILE['essence']}")
        guidance.append(f"Motto: {life_vector.VOICE_PROFILE['motto']}")
        
        guidance.append("\nğŸ’ CORE VALUES:")
        for value_key, value_data in life_vector.CORE_VALUES.items():
            guidance.append(f"  â€¢ {value_data['name']}")
        
        return "\n".join(guidance)
    
    def _calculate_stimulus(self, user_input):
        """
        ××—×©×‘ ×’×™×¨×•×™ ×¨×’×©×™ ××”×§×œ×˜ - ×¢×›×©×™×• ×¢× ×©×™×œ×•×‘ Life Vector.
        
        Returns:
            float: -1.0 (×©×œ×™×œ×™ ×××•×“) ×¢×“ 1.0 (×—×™×•×‘×™ ×××•×“)
        """
        text_lower = user_input.lower()
        
        positive = ["×ª×•×“×”", "××¢×•×œ×”", "×’××•×Ÿ", "×˜×•×‘", "×›×™×£", "××”×‘×ª×™", "××“×”×™×", "thanks", "great", "awesome", "×¦×•×“×§"]
        positive_score = sum(1 for w in positive if w in text_lower)
        
        negative = ["×˜×™×¤×©", "×’×¨×•×¢", "×¡×ª×•×", "×¨×¢", "××¢×¦×‘×Ÿ", "× ×××¡", "stupid", "bad", "annoying", "×œ× ××•×¢×™×œ"]
        negative_score = sum(1 for w in negative if w in text_lower)
        
        if positive_score > 0 and negative_score == 0:
            return min(0.5, positive_score * 0.2)
        elif negative_score > 0:
            return max(-0.4, -negative_score * 0.2)
        else:
            return 0.1
    
    def _get_learned_rules(self):
        """
        ××—×–×™×¨ ××ª ×”×—×•×§×™× ×©× ×œ××“×• (×-evolution.json)
        """
        evolution_path = os.path.join(DATA_DIR, "evolution.json")
        if os.path.exists(evolution_path):
            try:
                with open(evolution_path, "r", encoding="utf-8") as f:
                    rules = json.load(f)
                    return rules[-3:] if isinstance(rules, list) else []
            except:
                return []
        return []

brain = Consciousness()